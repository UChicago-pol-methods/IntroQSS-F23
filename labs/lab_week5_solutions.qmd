---
title: "Week 5 Lab"
author: Moksha Sharma
format: html
editor: visual
---

## Data Frames in R

A data frame is a two-dimensional, table-like of data structure in R. So far, the data structure with which we have been working has been vectors, and we can think of a data frame as a collection of equal-length vectors: each column is a different vector.

Recall that when we first started working with vectors, we discussed that you can only store on type of objects in one vector. In other words, a vector of characters could not also contain integers. Data frames, however, can store different classes of objects.

There are multiple ways of importing datasets into R so we can work with them. For example, if you have CSV file (CSV stands for "comma separated values", and it is basically a giant Excel sheet), you can use the `read.csv()` command. Of course, you have to be careful in specifying the path to the specific file (which can be a big pain to learn when you are starting with R!).

Various packages in `R` also contain data frames for illustrative purposes. We used the `iris` dataset in the first lab session, which is in the `tidyverse` package. For such datasets, we don't need to import them into our `R` session, because they are already loaded into R when we load the relevant package. As a result, we can just start working with them as needed.

The dataset we will used today can also be found in an `r` package (called `car`), but to maintain consistency with the problem set code, I uploaded it to Github as an `RData` file.

```{r}
#run this code to load the dataset
data_location <- "https://github.com/UChicago-pol-methods/IntroQSS-F23/raw/main/data/"
load(url(paste0(data_location, "prestige.Rdata")))
```

Let's look at what exactly we did here. First, we created an object to store the link to the class Github page (specifically, the page where all the data have been uploaded). The other important thing to note here is the `load()` function, which allows us to read an external file into the current `r` workspace.

Finally, the file we read in is an `RData` file, which is a file format in `r` that allows us to store multiple objects in one file. I stored only the dataset in this file, but on the problem set, when you load in the `RData` file provided by Andy, you will see that it loads in not just the dataset but also 2 vectors. These vectors correspond to vectors that are in the dataset you use in the problem set. As they are separate objects in your `R` workspace for the problem set, you can use them directly without having to reference the dataset.

## Prestige Dataset

Now, let's turn our attention to the dataset we are using today. It is called the "Prestige" dataset and is widely used for teaching linear regressions (in fact, in Linear Models, Mark Hansen Bobby Gulotty teach the original paper). The data is originally from 1970 and was used to measure how the perceived prestige of an occupation was related to how educated the employees in that industry were, how much money they made, and how many women worked in the industry.

1.  **How many variables are in the dataset?**

2.  **What does each row in the dataset represent?**

    1.  (note to self: compare with the package version of the dataset)

3.  **How many observations are there?**

4.  **What information do we have about each observation?**

## Linear Regression

A simple (perhaps too simplistic) way of defining a linear regression is as follows: when we linearly regress one variable (DV) on another (IV), we try to model the relationship between them as a straight line. In other words, we take the observed data and try to find a linear equation that best fits it ie minimises the sum of squared residuals .

Let's connect this to the class discussion of best linear predictor (BLP) towards the end of week 4. Just as it sounds, BLP is the linear predictor that best models the linear relationship between 2 variables. And since at least the 1800s, we have known that minimising the sum of the squared residuals - also know as the ordinary least squares (OLS) - gives us the BLP in the single predictor case. In other words, we obtain the BLP of *Y* given *X* by calculating the model parameters that minimise the sum of the squared residuals.

Let's try to appreciate visually why OLS is the BLP (iPad exercise).

## Computation of BLP:

Let's start by looking at what the linear model of a relationship between 2 variables would be. We know that some part of the IV would be explained by the DV and some of it would be explained by factors other than the DV. So our model will look as follows:

$$
\begin{equation}
Y = g(X) = a+bX
\end{equation}
$$

Of course, there are all sorts of things we don't know about this relationship, and to account for that, we add an error term. So the general form of an equation actually is $Y=a+bX+e$. However, we can't know what the error is, and by making a bunch of assumptions (that will be discussed in Linear Models), we can ignore it in the linear model that we estimate.

Here, $g(X)$ captures that *Y* is a function of *X.* What is the error in this context?

$$
\begin{align*}
\epsilon &= Y - (a+bX) \\
\implies \epsilon^2 &= [Y - (a+bX)]^2 \\
\implies E[\epsilon^2] &= E[Y - (a+bX)]^2
\end{align*}
$$

Since we are trying to minimise this error, we try to find values of $a$ and $b$ that would make the error as small as possible. Does the equation below look familiar?

$$
(\alpha, \beta) = \underset{(a,b) \in \mathbb{R}^2}{\arg\min} \, \mathrm{E}\,[\left(Y - (a + bX)\right)^2]
$$

To minimise a function, we differentiate it (with respect to what?) and set it to $0$. Doing so here gives us:

\- $\alpha = \textrm{E}[Y] - \beta \textrm{E}[X]$

\- $\beta = \frac{\textrm{Cov}[X, Y]}{\textrm{V}[X]}$

Without getting into too much detail, let's try to intuitively understand these parameters.

## Regression in R

There is a base R function that performs the OLS regression for us. Pick any variable from the Prestige dataset and use `lm()` to determine the linear relationship between your chosen variable and the perceived prestige of the occupations.

BEFORE you start, think about what the linear relationship should be. Then, get help from the function documentation to write your code.

```{r}
# your code here
summary(lm(prestige~women, prestige))
```

Fill out the equation below to get the functional form of this model:

$$
\text{prestige} = \beta_0+\beta_1()
$$

#### Interpretation:

#### **2 notes here:**

1.  If the variables you are using for your regression already exist as vectors in your environment, you don't have to call on the dataset to reference them.

2.  Or, you can also save the variables as vectors to run the regression.

3.  Or, you can also subset them out of the dataset using \$\$ signs.

```{r}
#your code here
summary(lm(prestige$prestige~prestige$women))
```

## Preview of Things to Come

#### Controls

Of course, prestige is affected by a lot of other factors, many of which are in the dataset. So we can regress "prestige" on multiple variables at the same time to see how they impact it individually. For instance, the presence of women does have a notable impact on the perceived prestige of the job, but it is confounded by the income associated with it. It is possible that if an industry is female-dominated, then that industry might be perceived as less prestigious. But industries dominated by women also tend to industries that pay less, so we would want to isolate the impact of women's presence by **controlling** for income. Such a regression is called a **multiple linear regression.**

$$
\text{prestige} = \beta_0+\beta_1() + \beta_2()
$$

```{r}
#your code here
summary(lm(prestige~income + women, prestige))
```

### Interaction Terms

You may also be interested in the **interaction** of 2 variables. This means that the effect of one DV on the IV may depend on the levels of another IV.

For instance, you may think that the effect of income on prestige depends on the presence of women in the industry or vice versa. Income and the presence of women obviously impact the prestige of a job. But it is also additionally possible that jobs that have more women are seen as more prestigious when the income associated with these jobs is higher. Working as a secretary is not seen as that prestigious because a lot of secretaries are women AND it is not a highly-paid job. However, a lot of psychologists are women AND they get paid a lot, which would contribute to the perceived prestige of the job.

```{r}
summary(lm(prestige~income + women + income*women, prestige))
```

### Pretty Plots

Finally, we can make pretty plots in `R` that model these relationships for us.

```{r}
library(ggplot2)
plot_1 <- ggplot(data = prestige,
                 mapping = aes(x = income,
                               y = prestige,
                               color = type)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Income", y="Prceived Prestige", "Type of Job") +
  theme_minimal()
  
plot_1
```
