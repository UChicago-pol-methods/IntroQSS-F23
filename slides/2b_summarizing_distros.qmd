---
title: "PLSC30500, Fall 2023"
subtitle: "Part 2. Summarizing distributions (part b)"
author: "Andy Eggers"
format: 
  revealjs:
    smaller: false
    slide-number: c/t
    css: ["default", "uchicago_pol_meth.css"]
    theme: simple
    echo: false
    embed-resources: true

---

```{r}
#| warning: false
#| message: false

library(tidyverse)
knitr::opts_chunk$set(dev = "ragg_png")
```




# Summarizing joint distributions



## Covariance  {.smaller}

$$\text{Cov}[X, Y] = E\left[  (X - E[X])(Y - E[Y]) \right]$$ 

. . . 

Intuitively, "Does $X$ tend to be above $E[X]$ when $Y$ is above $E[Y]$? (And by how much?)"

. . . 

$$
f(x,y) = \begin{cases}
1/10 & x = 0, y = 0 \\\
1/5 & x = 0, y = 1 \\\
1/5 & x = 1, y = 0 \\\
1/2 & x = 1, y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$

What is $E[X]$? What is $E[Y]$? 

. . .

Then compute expectation of $(X - E[X])(Y - E[Y])$ (function of two RVs) as above.


## Geometric interpretation {.smaller}

For each point $x,y$, construct a cube with width $x - E[X]$, height $y - E[Y],$ and depth $f(x,y)$. Add the "volumes".

::::{.columns}

:::{.column width="40%"}

$$
f(x,y) = \begin{cases}
1/10 & x = 0, y = 0 \\\
1/5 & x = 0, y = 1 \\\
1/5 & x = 1, y = 0 \\\
1/2 & x = 1, y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$

:::

:::{.column width="60%"}

```{r}
#| fig-height: 6
ex <- 6/10
ey <- 6/10
bind_rows(
  tibble(x = c(0, 0, ex, ex), y = c(ey, 0, 0, ey), lab = rep("A", 4), f = 1/10),
  tibble(x = c(0, 0, ex, ex), y = c(ey, 1, 1, ey), lab = rep("B", 4), f = 1/5),
  tibble(x = c(1, 1, ex, ex), y = c(ey, 0, 0, ey), lab = rep("C", 4), f = 1/5),
  tibble(x = c(1, 1, ex, ex), y = c(ey, 1, 1, ey), lab = rep("D", 4), f = 1/2)
  ) -> dat

dat |> 
  ggplot(aes(x = x, y = y, group = lab)) +
  geom_polygon(aes(fill = f)) + # , col = "black") + 
  geom_vline(xintercept = ex, lty = 2) + 
  geom_hline(yintercept = ey, lty = 2) +
  geom_point(data = tibble(x = c(0, 0, 1, 1), y = c(0, 1, 0, 1), lab = LETTERS[1:4]), size = 3) + 
  labs(alpha = "f(x, y)") + 
  coord_fixed() -> gg 

gg
```

:::

::::

## Alternative formulation

First formulation: 

$$\text{Cov}[X, Y] = E\left[  (X - E[X])(Y - E[Y]) \right]$$ 

. . . 

As with variance, an alternative formulation: 

$$\text{Cov}[X, Y] = E\left[XY\right] - E[X]E[Y]$$ 

. . .

Note: 

- if $E[X] = E[Y] = 0$ (e.g. if recentered), the two formulations look identical
- geometrically, can think in terms of areas of rectangles


. . . 

## Linearity of expectations, but not variances 

If $f$ is  a *linear function* or *linear operator*, then $f(x + y) = f(x) + f(y)$. (**Additivity** property.)

. . .

<br>

Recall that $E[X + Y] = E[X] + E[Y]$. 

Why? 

. . . 

<br>


But $\text{Var}[X + Y] \neq \text{Var}[X] + \text{Var}[Y]$

Why not? 


## Variance rule (non-linearity of variance) {.smaller}
### A different proof from A&R 2.2.3

$$\begin{aligned}
\text{Var}(X+Y) &= E[(X + Y - E[X + Y])^2] \\\
&= E[(X - E[X] + Y - E[Y])^2] \\\
&= E[(\tilde{X} + \tilde{Y})^2] \\\
&= E[\tilde{X}^2 + \tilde{Y}^2 + 2 \tilde{X} \tilde{Y}] \\\
&= E[\tilde{X}^2] + E[\tilde{Y}^2] + E[2 \tilde{X} \tilde{Y}] \\\
&= E[(X - E[X])^2] + E[(Y - E[Y])^2] + 2E[(X - E[X])(Y - E[Y])] \\\
&= \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X, Y)
\end{aligned}$$


<!-- Continue here: BLP, CEF from later lecture -->
