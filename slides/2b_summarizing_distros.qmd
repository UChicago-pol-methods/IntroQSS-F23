---
title: "PLSC30500, Fall 2023"
subtitle: "Part 2. Summarizing distributions (part b)"
author: "Andy Eggers"
format: 
  revealjs:
    smaller: false
    slide-number: c/t
    css: ["default", "uchicago_pol_meth.css"]
    theme: simple
    echo: false
    embed-resources: true

---

```{r}
#| warning: false
#| message: false

library(tidyverse)
knitr::opts_chunk$set(dev = "ragg_png")
```




# Summarizing joint distributions {background-image="assets/joint_distribution.png" background-size="750px" background-repeat="repeat"}


## Motivation

Suppose we have two RVs $X$ and $Y$ 

- number of heads in one coin flip and number of green balls drawn from urn in 6 tries
- age and height of randomly selected student
- whether randomly selected citizen served in military and supports a foreign war

. . .

We know the joint PMF/PDF $f(x, y)$ and joint CDF $F(x, y)$.

. . .

How can we summarize the relationship between $X$ and $Y$? 


## Covariance  {.smaller}

$$\text{Cov}[X, Y] = E\left[  (X - E[X])(Y - E[Y]) \right]$$ 

. . . 

Intuitively, "Does $X$ tend to be above $E[X]$ when $Y$ is above $E[Y]$? (And by how much?)"

. . . 

$$
f(x,y) = \begin{cases}
1/3 & x = 0, y = 0 \\\
1/6 & x = 1, y = 0 \\\
1/2 & x = 1, y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$

What is $E[X]$? What is $E[Y]$? 

. . .

Then compute expectation of $(X - E[X])(Y - E[Y])$ (function of two RVs) as above.



## Geometric representation (1) {.smaller}

Plot the points in $\text{Supp}[X, Y]$ on two axes with point size proportional to $f(x, y)$.

Divide the $x, y$ plane into quadrants defined by $x = E[X]$ and $y = E[Y]$. 

```{r}
dat <- tibble(x = c(0, 1, 1), y = c(0, 0, 1), fxy = c(1/3, 1/6, 1/2))
EX <- dat |> 
  summarize(sum(x*fxy)) |> 
  as.numeric()
EY <- dat |> 
  summarize(sum(y*fxy)) |> 
  as.numeric()
covar <- dat |> 
  summarize(sum(fxy*(x - EX)*(y - EY)))
dat |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(aes(size = fxy), show.legend = F) + 
  geom_vline(xintercept = EX, lty = 2) +
  geom_hline(yintercept = EY, lty = 2) + 
  theme_minimal() -> p
p
```




## Geometric representation (2) {.smaller}

For each point $(x, y) \in \text{Supp}[X, Y]$, create a rectangle with $(x,y)$ at one corner and $(E[X], E[Y])$ at the opposite corner.

Shade the rectangle green in quadrants I and III (where $(x - E[X])(y - E[X]) > 0$), otherwise red, with intensity proportional to $f(x,y)$.

Covariance (roughly) measures how much green vs red there is.

```{r}
#| fig-align: center
make_vertices_from_dat_row <- function(dat_row, label, EX, EY){
  expand_grid(x = c(dat_row$x, EX),
              y = c(dat_row$y, EY),
              label = label,
              col = ifelse((dat_row$x - EX)*(dat_row$y - EY) > 0, "green", "red"),
              fxy = dat_row$fxy)[c(1,2,4,3,1),]
}
datt <- bind_rows(
  make_vertices_from_dat_row(dat[1,], "A", EX, EY),
  make_vertices_from_dat_row(dat[2,], "B", EX, EY),
  make_vertices_from_dat_row(dat[3,], "C", EX, EY)
)
p +
  geom_polygon(data = datt, aes(fill = col, alpha = fxy), show.legend = F) + 
  scale_fill_manual(values = c("green", "red")) + 
  labs(title = str_c("Covariance: ", round(covar, 2)))
```



## Geometric representation (3) {.smaller}

<!-- another example -- maybe make a function? --> 

```{r}
#| fig-align: center
make_cov_plot <- function(pmf){
  EX <- pmf |> 
    summarize(sum(x*fxy)) |> 
    as.numeric()
  EY <- pmf |> 
    summarize(sum(y*fxy)) |> 
    as.numeric()
  covar <- pmf |> 
    summarize(sum(fxy*(x - EX)*(y - EY)))  
  
  pmf |> 
    ggplot(aes(x = x, y = y)) + 
    geom_point(aes(size = fxy), show.legend = F) + 
    geom_vline(xintercept = EX, lty = 2) +
    geom_hline(yintercept = EY, lty = 2) + 
    theme_minimal() -> p
  
  sto <- list()
  for(i in 1:nrow(pmf)){
    sto[[i]] <- make_vertices_from_dat_row(pmf[i,], str_c("A", i), EX = EX, EY = EY)
  }
  
  rect_data <- bind_rows(sto)
  
  p +
    geom_polygon(data = rect_data, aes(group = label, fill = col, alpha = fxy), show.legend = F) + 
    scale_fill_manual(values = c("green", "red")) + 
    labs(title = str_c("Covariance: ", round(covar, 2)))

}

pmf <- tibble(x = c(0, 1, 0), y = c(0, 0, 1), fxy = c(1/3, 1/6, 1/2))
make_cov_plot(pmf)
```

## Geometric representation (4) {.smaller}


```{r}
n <- 7
set.seed(123)
pmf <- tibble(x = runif(n),
              fxy_raw = runif(n)) |> 
  mutate(y = x + rnorm(n, sd = 1),
         fxy = fxy_raw/sum(fxy_raw)) 

make_cov_plot(pmf)
```

## Geometric representation (5) 


```{r}
n <- 7
pmf <- tibble(x = runif(n),
              fxy_raw = runif(n)) |> 
  mutate(y = -x + rnorm(n, sd = 1),
         fxy = fxy_raw/sum(fxy_raw)) 

make_cov_plot(pmf)
```

## Alternative formulation

First formulation: 

$$\text{Cov}[X, Y] = E\left[  (X - E[X])(Y - E[Y]) \right]$$ 

. . . 

As with variance, an alternative formulation: 

$$\text{Cov}[X, Y] = E\left[XY\right] - E[X]E[Y]$$ 

. . .

Note: 

- if $E[X] = E[Y] = 0$ (e.g. if recentered), the two formulations look identical
- geometrically, can think in terms of areas of rectangles


. . . 

## Linearity of expectations, but not variances 

If $f$ is  a *linear function* or *linear operator*, then $f(x + y) = f(x) + f(y)$. (**Additivity** property.)

. . .

<br>

Recall that $E[X + Y] = E[X] + E[Y]$. 

Why? 

. . . 

<br>


But $\text{Var}[X + Y] \neq \text{Var}[X] + \text{Var}[Y]$

Why not? 


## Variance rule (non-linearity of variance) {.smaller}
### A different proof from A&R 2.2.3

$$\begin{aligned}
\text{Var}(X+Y) &= E[(X + Y - E[X + Y])^2] \\\
&= E[(X - E[X] + Y - E[Y])^2] \\\
&= E[(\tilde{X} + \tilde{Y})^2] \\\
&= E[\tilde{X}^2 + \tilde{Y}^2 + 2 \tilde{X} \tilde{Y}] \\\
&= E[\tilde{X}^2] + E[\tilde{Y}^2] + E[2 \tilde{X} \tilde{Y}] \\\
&= E[(X - E[X])^2] + E[(Y - E[Y])^2] + 2E[(X - E[X])(Y - E[Y])] \\\
&= \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X, Y)
\end{aligned}$$


## Covariance, correlation, and independence  {.smaller}

The **correlation** of two RVs $X$ and $Y$ with $\sigma[X] > 0$ and $\sigma[Y] > 0$ is 

$$ \rho[X, Y] = \frac{\text{Cov}[X, Y]}{\sigma[X] \sigma[Y]}$$

. . .

A rescaled, **scale-invariant** version of covariance:  $\rho[X, Y] = \rho[aX, bY]$ for $a, b > 0$

. . . 

If $X$ and $Y$ are independent, then 

- zero covariance
- zero correlation
- $V[X + Y] = V[X] + V[Y]$


## Conditional expectation: definition (2.2.10)

Discrete version: 

$$E [Y | X = x] = \sum_y y f_{Y|X}(y | x)$$ 

i.e. the expectation of $Y$ at some $x$. 

. . .

Continuous version: 

$$E [Y | X = x] = \int_{-\infty}^\infty y f_{Y|X}(y | x) \, dy$$


## Conditional expectation: illustration  {.smaller}

Suppose $f_X(x)$ is uniform, and $f_{Y|X}(y|x)$ normal, with mean of $3x + 8x^2 - 3x^3$ and variance of $9$.

Then what is $E [Y | X = 1]$?


```{r}
#| fig-align: center
n <- 1000
dat <- tibble(x = runif(n, min = 0, max = 2.75)) |> 
  mutate(mu = 3*x + 8*x^2 - 3*x^3,
         y = rnorm(n, mean = mu, sd = 3))
p <- dat |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5, size = 2) +
  theme_bw() + 
  geom_line(aes(y = mu), col = "red", lwd = 2)
p
```




## Conditional variance {.smaller}

Two formulations: 

$$V[Y | X = x] = E[(Y - E[Y | X =x])^2 | X = x]$$
$$V[Y | X = x] = E[Y^2 | X = x] - E[Y | X =x]^2$$


```{r}
#| fig-align: center
p
```


## Conditional variance (2) {.smaller}


Two formulations: 

$$V[Y | X = x] = E[(Y - E[Y | X =x])^2 | X = x]$$

$$V[Y | X = x] = E[Y^2 | X = x] - E[Y | X =x]^2$$

```{r, echo = F}
#| fig-align: center
tibble(x = runif(n, min = 0, max = 2.75)) |> 
  mutate(mu = 3*x + 8*x^2 - 3*x^3,
         y = rnorm(n, mean = mu, sd = 1 + x)) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5, size = 2) +
  theme_bw() + 
  geom_line(aes(y = mu), col = "red", lwd = 2)
```



## Conditional Expectation Function (CEF) {.smaller}

Conditional expectation $E[Y | X = x]$ is for a specific $x$.

Conditional expectation function (CEF) $E[Y | X]$ is for all $x$.

. . . 

Suppose again $f_X(x)$ is uniform, and $f_{Y|X}(y|x)$ normal, with mean of $3x + 8x^2 - 3x^3$ and variance of $9$.

Then what is $E [Y | X ]$?

```{r}
#| fig-height: 3.5
p
```




## CEF as best predictor

The CEF $E[Y | X]$ is the expectation of $Y$ at each $X$.

. . .

We already established that the expectation/mean is the best (in MSE sense) predictor.

. . .

So CEF is the best possible way to use $X$ to predict $Y$.  (See Theorem 2.2.20.)




## Law of iterated expectations

**Theorem 2.2.17**. *Law of iterated expectations*

For random variables $X$ and $Y$,

$$E[Y] = E[E[Y | X]]$$

. . . 

In words: to get the average of $Y$, you can take the average of $Y$ within levels of $X$ and then average those according to the distribution of $X$.

[(Very similar to the law of total probability.)]{.gray}



## LIE: An intuitive example

A population is 80% female and 20% male.

The average age among females ([$E[Y | X = 1]$]{.gray}) is 25. The average age among males [$E[Y | X = 0]$]{.gray} is 20.

What is the average age in the population [$E[Y]$]{.gray}?

. . .

$$E[E[Y | X]] = .8 \times 25 + .2 \times 20 = 24$$

. . .

See homework for another example.



## LIE: another example

```{r, echo = F}
p
```



## LIE: another example (2)

```{r, echo = F}
n <- 5000
tibble(x = rnorm(n, mean = 3, sd = 1.5)) |> 
  mutate(mu = 3*x + 8*x^2 - 3*x^3,
         y = rnorm(n, mean = mu, sd = 3)) |> 
  filter(x > 0 & x < 2.75) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5, size = 2) +
  theme_bw() + 
  geom_line(aes(y = mu), col = "red", lwd = 2)
```


##  How LIE is used in causal inference (preview) {.smaller}

Suppose we want to measure the average effect of participating in a program (e.g. job training, voter education, military mobilization).

. . . 

Call $\tau_i$ the [(unobservable)]{.gray} treatment effect for unit $i$. We want the **average treatment effect** (ATE), $E[\tau_i]$.

. . . 

Suppose comparing all participants to all non-participants produces a **biased** estimate, but we can get a good estimate of the effect in each subgroup defined by participant background $(X)$.

. . . 

Then we can combine subgroup estimates because (by LIE) $E[\tau_i] = E[E[Y \mid X]]$. 





## Law of total variance

$$V[Y] = E[V[Y|X]] + V[E[Y|X]]$$

. . . 

```{r, echo = F}
p
```

<!-- 
## Law of total variance (2)

This will be approximately true of a sample (esp. when we know the CEF):

```{r}
head(dat, 4)
```

```{r}
dat |> 
  summarize(var(y), var(y - mu), var(mu))
```

Sample variance of $Y$ equals (approx) sample variance of errors plus sample variance of CEF.
--> 




## Best linear predictor (BLP) {.smaller}


Suppose we want to predict $Y$ using $X$, and we focus on a linear predictor, i.e. a function of the form $\alpha + \beta X$.

. . . 

The best (minimum MSE) predictor satisfies

$$(\alpha, \beta) = \underset{(a,b) \in \mathbb{R}^2}{\arg\min} \, \mathrm{E}\,[\left(Y - (a + bX)\right)^2]$$

. . . 

The solution (see Theorem 2.2.21) is 

- $\beta = \frac{\textrm{Cov}[X, Y]}{\textrm{V}[X]}$
- $\alpha = \textrm{E}[Y] - \beta \textrm{E}[X]$ 

. . . 

So we could obtain the BLP from a joint PMF. (See homework.)



## BLP predicts CEF {.smaller}

::::{.columns}

:::{.column column-width="50%"}
Above, we were looking for best linear predictor (BLP) of $Y$ as function of $X$: 

$$(\alpha, \beta) = \underset{(a,b) \in \mathbb{R}^2}{\arg\min} \, \mathrm{E}\,[\left(Y - (a + bX)\right)^2]$$

:::


:::{.column column-width="50%"}

Same answer if you look for the best linear predictor of the CEF $E[Y | X]$:

$$(\alpha, \beta) = \underset{(a,b) \in \mathbb{R}^2}{\arg\min} \, \mathrm{E}\,[\left(\mathrm{E}[Y|X] - (a + bX)\right)^2]$$


:::
::::


```{r, echo = F, fig.height = 3.5}
p + 
  geom_smooth(method = lm)
```

