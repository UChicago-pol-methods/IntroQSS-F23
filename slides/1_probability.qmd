---
title: "PLSC30500, Fall 2023"
subtitle: "Part 1. Probability"
author: "Andy Eggers"
format: 
  revealjs:
    smaller: false
    slide-number: c/t
    css: ["default", "uchicago_pol_meth.css"]
    theme: simple
    echo: false
    embed-resources: true

---

```{r}
#| warning: false
#| message: false

library(tidyverse)

```


# Motivation & big picture 

## Coin flips and urn problems

You will see a lot of problems about coin flips and selecting balls from urns.

What does this have to do with social science? 


![](assets/urn.png){width=60% fig-align="center"}


## Sampling


Sometimes a coin flip, an urn, or a similar device actually determines which **units/observations** we see: who gets selected for a survey.

The urn problems help us understand how the **sample** might differ from the **population** (and thus how certain we can be about characteristics of the population using the sample).



## Treatment assignment

Sometimes a coin flip, an urn, or a similar device actually determines which units/observations get a random **treatment**, e.g. in a **randomized experiment** or the Vietnam draft lottery.

The urn problems help us compare differences we see between treatment and control units to differences we might see by chance if the treatment had no effect.


## Urns as metaphors


Even when there was no random selection (e.g. data on all countries) we can *act as if there was*, or act as if the dependent variable (e.g. revolution) has a random component. 

Then the urn problems again help us compare the "sample" to the "population", or observed reality to what might have happened in an alternate history.


## Probability vs statistical inference {.smaller}

::::{.columns}

::: {.column width="50%"}

:::{.fragment}

In probability problems, we know what's in the urn and we want to describe the possible **draws**.

![](assets/urn.png){width=60% fig-align="center"}

:::
:::


::: {.column width="50%"}

:::{.fragment}
In many statistics problems, we have one draw and we want to speculate what might be in the urn (i.e. population).

![](assets/urn_question_marks.png){width=80% fig-align="center"}
:::
:::

::::


# Probability foundations

## What is probability? {.smaller}



::::{.columns}

::: {.column width="50%"}

:::{.fragment}

A **random generative process** is a repeatable mechanism that can select an outcome from a set of possible outcomes.


Each **draw** or **realization** of the process may be uncertain (to the typical observer), but the frequency of each **event** can be described.  

e.g. flipping a coin, rolling a die, drawing a ball from an urn.

:::
:::


::: {.column width="50%"}

:::{.fragment}
![](assets/urn.png){width=50% fig-align="center"}



:::
:::

::::


. . .


**Frequentist definition of probability**: The **probability** of an event (e.g. "green ball is chosen") is the proportion of many, many draws producing that event. 

. . . 

**Bayesian definition of probability**: The **probability** of an event is an observer's degree of belief that the event will happen or has happened. Logical and subjective variants.

 

## Aside on mathematical notation


![](assets/am_math_appx.png){width=20% fig-align="center"}


## Sample space {.smaller}

Sample space $\Omega$ ("Omega") is the set of all possible outcomes of the random generative process. Each element $\omega$ ("omega") is a unique outcome of the process.

. . .

For a coin flip, $\Omega = \{H, T\}$; $\omega \in \{H, T\}$.

. . .

For a single roll of a six-sided die, $\Omega = \{1, 2, 3, 4, 5, 6\}$.

. . .

How about for a single roll of two six-sided die? 

. . . 

$$\Omega = \{(x, y) \in \mathbb{Z}^2 : 1 \leq x \leq 6, 1 \leq y \leq 6 \}$$
("Set-builder notation", used w/o explanation at A&M p. 5)
 
 
:::{.notes}
All states of the world that can result from the process.
:::
 
## Sample space (2)

$$\Omega = \{(x, y) \in \mathbb{Z}^2 : 1 \leq x \leq 6, 1 \leq y \leq 6 \}$$

. . .

i.e., $\Omega = \{(1,1), (1,2), \ldots (1,6), (2,1), (2,2), \ldots, (6,6) \}$

```{r}
#| fig-height: 3
#| fig-width: 3
#| echo: false
#| fig-align: center
expand_grid(x = 1:6, y = 1:6) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point() +
  theme_bw()
```

## Event space {.smaller}

An **event** is a collection of outcomes to which we want to assign a probability. (A subset of the sample space.)

. . .

Examples: 

- rolling a 3 [(Here, an outcome is an event)]{.gray}
- rolling an even number
- in an election, a tie for first between candidates $a$ and $b$

. . .

An **event space** $S$ is a set of events composed in a particular way (for technical reasons): 

- a set of events of interest [(e.g. $A$ = $a$ wins, $B$ = $b$ wins, $T$ = $a$ and $b$ tie)]{.gray}
- their complements [($A^C$ = $a$ doesn't win, $B^C$ = $b$ doesn't win, $T^C$ = $a$ and $b$ don't tie)]{.gray}
- the **union** of each subset of events [(e.g. $A \cup B$ = $a$ wins OR $b$ wins, $A \cup T$ = $a$ wins or ties, $\ldots$)]{.gray}



## Probability measure & Kolmogorov axioms {.smaller}

A **probability measure**  is a function $P : S \rightarrow \mathbb{R}$ that assigns a probability to every event in the event space. 

. . .

**Kolmogorov axioms**: $(\Omega, S, P)$ is a **probability space** if it satisfies the following: 

- Non-negativity: $\forall A \in S$, $P(A) \geq 0$  [(probabilities are positive)]{.dark-gray}
- Unitarity: $P(\Omega) = 1$ [(probability that something happens is 1)]{.dark-gray}  
- Countable additivity: if $A_1, A_2, A_3, \ldots \in S$ are pairwise disjoint, then 

$$P(A_1 \cup A_2 \cup A_3 \cup \ldots ) = P(A_1) + P(A_2) + P(A_3) + \ldots = \sum_i P(A_i) $$
[(for events that cannot co-occur, the probability of one of the event occurring is the sum of the individual probs)]{.dark-gray}


## Kolmogorov axioms (2) {.smaller}

Recall the axioms: 

- Non-negativity: $\forall A \in S$, $P(A) \geq 0$  [(probabilities are positive)]{.dark-gray}
- Unitarity: $P(\Omega) = 1$ [(probability that something happens is 1)]{.dark-gray}  
- Countable additivity: if $A_1, A_2, A_3, \ldots \in S$ are pairwise disjoint, then 

$$P(A_1 \cup A_2 \cup A_3 \cup \ldots ) = P(A_1) + P(A_2) + P(A_3) + \ldots = \sum_i P(A_i) $$

. . .

Explain why, if $(\Omega, S, P)$ is a probability space

- $\forall A \in S$, $0 \leq P(A) \leq 1$
- $\forall A \in S$, $P(A^C) = 1 - P(A)$

(See Theorem 1.1.4.)


## An aside on what we're doing

The goal is a strong system of understanding where every statement follows from 

- definitions [(what we mean by $X$)]{.gray}
- axioms [(assertions taken to be self-evident requirements)]{.gray}
- assumptions [(assertions that usefully restrict/simplify)]{.gray}
- logical argument following from above

. . . 

Every statement is supported, no unnecessary assumptions/definitions.


## Joint probability and the addition rule  {.smaller}

**Def 1.1.5**: For $A, B \in S$, the **joint probability** of $A$ and $B$ is the probability that both $A$ and $B$ happen, i.e. $P(A \cap B)$

. . . 

```{tikz venn}
#| fig-width: 5 
#| fig-align: center
\begin{tikzpicture}
    % Draw the rectangles
    \draw (0,0) rectangle (5,3);
    \fill[gray, opacity=0.25] (1.75,1.5) circle (1.2);
    \draw (1.75,1.5) circle (1.2);
    \fill[gray, opacity=0.5] (3.25,1.5) circle (1.2);
    \draw (3.25,1.5) circle (1.2);

    % Label the circles
    \node at (1.5,1.5) {$A$};
    \node at (3.5,1.5) {$B$};
    \node at (2.5,1.5) {$A \cap B$};
\end{tikzpicture}
```


. . . 

**Addition rule**: For $A, B \in S$, 

$$ P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

## Assumptions behind our use of Venn diagram 

```{tikz venn}
#| fig-width: 6 
#| fig-align: center
```

- think of each pixel in the rectangle as an outcome $\omega$ in the sample space $\Omega$
- each outcome has equal probability
- $A$ and $B$ are events (sets of outcomes)


## Conditional probability 

**Def 1.1.8**: For $A, B$ with $P(B) > 0$, the **conditional probability** of $A$ given $B$ is 

$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$

```{tikz venn}
#| fig-width: 4 
#| fig-align: center
```

. . . 

Read $P(A \mid B)$ as "probability of $A$ given $B$". 

## Equivalently: product rule

For $A, B$ with $P(B) > 0$, 

$$P(A \cap B) = P(B) P(A | B)$$

```{tikz venn}
#| fig-width: 4 
#| fig-align: center
```

. . .

[Here, product rule follows from definition of conditional probability; in logical approach (Cox's Theorem) it follows from consistency axioms.]{.smaller .gray} 

## Partition

If $A_1, A_2, \ldots \in S$ is nonempty and pairwise disjoint, and $\Omega = A_1 \cup A_2 \cup \ldots$, then  $A_1, A_2, \ldots$ is a **partition** of $\Omega$. 

. . . 

```{tikz ltp}
#| fig-width: 6 
#| fig-align: center
\begin{tikzpicture}
    % Define the widths of the strips
    \def\stripwidths{{1, 2, 1.5, 2.5}}
    
    % Set the total width and height of the rectangle
    \pgfmathsetmacro\totalwidth{7}
    \pgfmathsetmacro\totalheight{4}
    
    % Draw the rectangle
    \draw (0, 0) rectangle (\totalwidth, \totalheight);
    
    % Draw the vertical strips
    \foreach \x/\j in {0/1, 1/2, 3/3, 4.5/4} {
        \draw (\x, 0) -- ++(0, \totalheight) node[very near end, right=5pt] {$A_\j$};
    }
\end{tikzpicture}
```



## Law of total probability 

If $\{A_1, A_2, \ldots \}$ is a partition of $\Omega$ and $P(A_i) > 0 \, \forall \, i$, then 

$$P(B) = \sum_i P(B\cap A_i) = \sum_i P(B \mid A_i) P(A_i)$$

. . . 

```{tikz ltp2}
#| fig-width: 6 
#| fig-align: center
\begin{tikzpicture}
    % Define the widths of the strips
    \def\stripwidths{{1, 2, 1.5, 2.5}}
    
    % Set the total width and height of the rectangle
    \pgfmathsetmacro\totalwidth{7}
    \pgfmathsetmacro\totalheight{4}
    
    % Draw the rectangle
    \draw (0, 0) rectangle (\totalwidth, \totalheight);
    
    % Draw the vertical strips
    \foreach \x/\j in {0/1, 1/2, 3/3, 4.5/4} {
        \draw (\x, 0) -- ++(0, \totalheight) node[very near end, right=5pt] {$A_\j$};
    }

    % Draw circle for event B
    \fill[gray, opacity=0.5] (3.25,1.75) circle (1.5);
    \draw (3.25,1.75) circle (1.5);

    % Label the circle
    \node at (3.5,1.75) {$B$};

\end{tikzpicture}
```

## Independence of events

**Definition**: Events $A, B \in S$ are **independent** if $P(A \cap B) = P(A)P(B)$

. . .

```{tikz ind}
#| fig-width: 6 
#| fig-align: center
\usetikzlibrary{decorations.pathreplacing}
\begin{tikzpicture}

    % Draw the overall rectangle
    \draw (0, 0) rectangle (5, 3);
    
    % Draw the event rectangles
    \fill[blue, opacity=0.5] (0,0) rectangle (2, 3);
    \fill[red, opacity=0.2] (0,2) rectangle (5, 3);

    % Event boundaries 
    \draw (2, 0) -- ++(0, 3);
    \draw (0, 2) -- ++(5, 0);

    % Label the events
    \node at (1,1.5) {$A$};
    \node at (2.5,2.5) {$B$};

    \draw [decorate, decoration={brace, amplitude=5pt, mirror}] (5,2) -- (5,3) node[midway, right=5pt] {$P(B)$};
    \draw [decorate, decoration={brace, amplitude=5pt, mirror}] (0,0) -- (2,0) node[midway, below=5pt] {$P(A)$};

\end{tikzpicture}
```




## Independence and conditional probability 

**Theorem 1.1.16** For $A, B \in S$ with $P(B) > 0$, $A$ and $B$ are independent if and only if $P(A \mid B) = P(A).$ 

. . .

$A$ and $B$ are independent $\iff P(A \cap B) = P(A)P(B)$  [(definition of independence)]{.gray}

. . . 

$A$ and $B$ are independent $\iff P(A \mid B) P(B) = P(A)P(B)$  [(product rule)]{.gray}

. . . 

$A$ and $B$ are independent $\iff P(A \mid B) = P(A)$  [(divide both sides by $P(B)$)]{.gray}


## Independent? 

(Recall: events $A$ and $B$ independent means $P(A \cap B) = P(A)P(B)$ and $P(A|B) = P(A)$)

Pick a student at random from the university. Are $A$ and $B$ independent in the following examples?

- $A$ = student is undergraduate, $B$ = student is under 22 years old
- $A$ = student is studying biology, $B$ = student is breathing
- $A$ = student is woman, $B$ = student has brown hair



## Random variables 

Recall a random generative process produces an outcome $\omega \in \Omega$; events (e.g. $A$, $B$) are sets of outcomes. 

. . . 

A random variable $X$ is a function that maps each outcome $\omega$ to an event expressed as a number. ["Random variables are real-valued functions of outcomes" (A&M p. 38)]{.gray}

. . . 

e.g. $X(\omega) = 1$ means that the outcome $\omega$ is part of the event 1.

. . . 

Usually we just write $X = 1$ and $\text{Pr}[X = 1]$. [(cf $P(A)$ for events)]{.gray}

. . .

[Can also think of $X$ as a random process that produces numbers as outcomes, but A&M's way distinguishes the random process itself from the researcher's representation of it in numbers.]{style="font-size: 50%; line-height: 10%"}


## Functions/operators of random variables {.smaller}

Since a random variable $X$ produces numbers, we can apply **functions**: e.g. $X^2$, $\text{log}(X)$, generically $g(X)$

This gives us a new number for each $\omega$ -- a new random variable. 

<br>

. . .

We will also want to describe characteristics of random variables using **operators**: e.g. $E[X]$ (**expectation**) and $V[X]$ (**variance**). 

This gives us a number to describe $X$ -- **not** a new random variable. [(Though we may estimate these from samples, producing random variables, e.g. **sample mean**, **sample variance**.)]{.gray}




## Discrete random variables and the PMF {.smaller}

A random variable $X$ is **discrete** if its range $X(\Omega)$ is a **countable set**. For example, $\{1,2,3\}$, $\{1,2,3, \ldots\}.$

. . .

A discrete RV can be fully described by a **probability mass function** (PMF): $$f(x) = \text{Pr}[X = x], \forall x \in \mathbb{R}.$$

. . .

For example, the number of heads in two flips of a fair coin:

$$
f(x) = \begin{cases}
1/4 & x = 0 \\\
1/2 & x = 1 \\\
1/4 & x = 2 \\\
0 & \text{otherwise}
\end{cases}
$$

## The CDF 

The **cumulative distribution function** (CDF) of a random variable $X$ is 

$$ F(x) = \text{Pr}[X \leq x], \forall x \in \mathbb{R}$$
The CDF is another way to fully describe a random variable.

. . .

For the coin flip example, 

$$
F(x) = \begin{cases}
0 & x < 0 \\\
1/4 & 0 \le x < 1 \\\
3/4 & 1 \le x < 2 \\\
1 & x \ge 2
\end{cases}
$$

## The PMF and CDF for number of heads in two coinflips

```{r}
#| echo: false
plotdata <- tibble(
  x = c(-1, 0, 1, 2),
  xend = c(0, 1, 2, 3),
  `f(x)` = c(0, 1/4, 1/2, 1/4),
  `F(x)` = cumsum(`f(x)`)
)
```

:::: {.columns}

:::{.column width="50%"}
```{r coinflip_plotCDF}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#| echo: false
ggplot(plotdata, aes(x = x, y = `f(x)`)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.5, 2.5),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0,0), xend = x, yend = `f(x)`)) +
  labs(main = 'PMF of X (number of heads in 2 fair coin flips)',
       x = "x (number of heads)")
```
:::

:::{.column width="50%"}
```{r coinflip_plotPMF}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#| echo: false
ggplot(plotdata, aes(x = x, y = `F(x)`)) +
  geom_segment(aes(x = x, y = `F(x)`, xend = xend, yend = `F(x)`)) + 
  geom_point() +
  geom_point(aes(x = xend, y = `F(x)`), shape= 21, fill = 'white') +
  coord_cartesian(xlim = c(-0.5, 2.5),
                  ylim = c(0,1)) +
  labs(title = 'CDF of X (number of heads in 2 fair coin flips)',
       x = "x (number of heads)")
```
:::

::::


## Continuous random variables {.smaller}

If a random variable could take on a **continuum** of values [(i.e. $X(\Omega)$ includes some interval of the real line)]{.gray}, then we say it is **continuous**.

. . . 


```{r}
#| echo: false
norm_plotdata <- tibble(x = seq(-4, 4, by = .01)) |> 
  mutate(`f(x)` = dnorm(x),
         `F(x)` = pnorm(x))
```

:::: {.columns}

:::{.column width="50%"}
```{r norm_plotPMF}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#| echo: false
ggplot(norm_plotdata, aes(x = x, y = `f(x)`)) +
  geom_line() +
  labs(title = 'PDF of X',
       x = "x")
```
:::

:::{.column width="50%"}
```{r norm_plotCDF}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#| echo: false
ggplot(norm_plotdata, aes(x = x, y = `F(x)`)) +
  geom_line() +
  labs(title = 'CDF of X',
       x = "x")
```
:::

::::


## Integrating PDF to get probability of event in interval 

Probability **density** function (PDF) written $f(x)$, CDF written $F(x)$.

. . .

CDF is integral of PDF below $x$: 

$$F(x) = \text{Pr}[X \leq x] = \int_{-\infty}^x f(u) du$$

. . .

$$\text{Pr}[a \leq X \leq b] = \int_{a}^b f(u) du = F(b) - F(a)$$

## Integration visually 

```{r}
#| echo: false
integration_data <- tibble(x = c(norm_plotdata$x, rev(norm_plotdata$x)),
                           `f(x)` = c(norm_plotdata$`f(x)`, rep(0, nrow(norm_plotdata))))
```


:::: {.columns}

:::{.column width="50%"}
```{r norm_plot_CDFintegral}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#| echo: false

a <- -.6
ggplot(norm_plotdata, aes(x = x, y = `f(x)`)) +
  geom_line() +
  geom_polygon(data = integration_data |> filter(x < a),
               fill = "lightblue", alpha = .5) +
  geom_line(data = tibble(x = c(a, a), `f(x)` = c(0, dnorm(a)))) + 
  labs(main = 'PDF of X',
       x = "x") + 
  scale_x_continuous(breaks = c(-4, -2, a, 0, 2, 4), labels = c(-4, -2, "a", 0, 2, 4))
```
:::

:::{.column width="50%"}
```{r norm_plot_interval_integral}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#| echo: false
a <- -.9
b <- .6
ggplot(norm_plotdata, aes(x = x, y = `f(x)`)) +
  geom_line() +
  geom_polygon(data = integration_data |> filter(x < b & x > a),
               fill = "lightblue", alpha = .5) +
  geom_line(data = tibble(x = c(a, a), `f(x)` = c(0, dnorm(a)))) + 
  geom_line(data = tibble(x = c(b, b), `f(x)` = c(0, dnorm(b)))) + 
  labs(main = 'CDF of X',
       x = "x") + 
  scale_x_continuous(breaks = c(-4, -2, a, 0, b, 2, 4), labels = c(-4, -2, "a", 0, "b", 2, 4))
```
:::

::::


## Uniform random variable 


```{r}
#| echo: false
unif_plotdata <- tibble(x = seq(-.5, 1.5, by = .01)) |> 
  mutate(`f(x)` = dunif(x),
         `F(x)` = punif(x))
```

:::: {.columns}

:::{.column width="50%"}
```{r unif_plotPMF}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#| echo: false
ggplot(unif_plotdata, aes(x = x, y = `f(x)`)) +
  geom_line() +
  labs(title = 'PDF of X',
       x = "x")
```
:::

:::{.column width="50%"}
```{r unif_plotCDF}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#| echo: false
ggplot(unif_plotdata, aes(x = x, y = `F(x)`)) +
  geom_line() +
  labs(title = 'CDF of X',
       x = "x")
```
:::

::::




## Bivariate relationships

We might have two random variables -- e.g. flip a coin and draw a ball from an urn.

. . . 

Or, in a data analysis problem we could view two characteristics as random variables, e.g. 

- age of respondent and whether or not respondent voted
- GDP of country and how democratic it is


## Describing bivariate relationships

We can describe two random variables $X$ and $Y$ with 

- **joint PMF**: 

$$
f(x,y) = \textrm{P}[X=x, Y=y], \forall x, y \in \mathbb{R}
$$

- **joint CDF**:

$$
F(x,y) = \textrm{P}[X \leq x, Y \leq y], \forall x, y \in \mathbb{R}
$$

## Describing bivariate relationships (2) 

Let $X$ denote number of heads in **two** tosses of a fair coin.

Let $Y$ denote number of heads in **one** toss of a fair coin.


$$
f(x, y) = \begin{cases} 
1/8 & x = 0, y = 0 \\\
1/8 & x = 0, y = 1 \\\
1/4 & x = 1, y = 0 \\\
1/4 & x = 1, y = 1 \\\
1/8 & x = 2, y = 0 \\\
1/8 & x = 2, y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$


## "$X$-by-$Y$" representation of joint PMF

Let $X$ denote number of heads in **two** tosses of a fair coin.

Let $Y$ denote number of heads in **one** toss of a fair coin.

Then $\text{Pr}[X = x, Y = y]$ is given by this table: 


```{r plainjoint}
#| echo: false

tib <- tibble(X = c("x = 0", "x = 1", "x = 2"), 
              `Y0` = c("1/8", "1/4", "1/8"), 
              `Y1` = c("1/8", "1/4", "1/8"))
kableExtra::kbl(tib, col.names = c("", "y = 0", "y = 1"), booktabs = T) |> 
  kableExtra::column_spec(1,border_right = T) |> 
  kableExtra::column_spec(1,bold = T) |> 
  kableExtra::row_spec(1, extra_css = "border-top: 1px solid")
```


## Graphical representation of joint PMF

```{r rglpmf, webgl = T}
#| echo: false
#| message: false
#| warning: false
#| output: false

library(rgl)
setupKnitr(autoprint = TRUE)

dat <- tribble(~x, ~y, ~z,
        -1, -1, 0,
        3, 2, 0,
        0, 0, 1/8,
        0, 1, 1/8,
        1, 0, 1/4,
        1, 1, 1/4,
        2, 0, 1/8,
        2,1, 1/8)

# Initialize the 3D plot
open3d()

aspect3d(x=3, y=3 , z =30)

# par3d(windowRect = c(20, 30, 800, 800))

# Create the 3D plot with lines
for (i in 1:nrow(dat)) {
  points3d(x = dat$x[i], y = dat$y[i], z = dat$z[i])
  segments3d(
    x = rep(dat$x[i], 2),
    y = rep(dat$y[i], 2),
    z = c(0, dat$z[i]),
    col = "blue",  # You can choose your desired color
    lwd = 2        # Adjust the line width as needed
  )
}

# Set axis labels
axes3d("bbox")
title3d(xlab = "x", ylab = "y", zlab = "f(x,y)")

rgl.close()

```

## Marginal PMF 

Recall: A joint PMF $f(x, y) = \text{Pr}[X = x, Y = y]$ describes the distribution of two discrete RVs $X$ and $Y$.

. . .

We can also talk about the **marginal PMF** of one of the variables: 

$$f_Y(y) = \text{Pr}[Y = y] = \sum_{x \in \text{Supp}[X]} f(x, y), \forall y \in \mathbb{R}.$$

Basically, this describes the distribution of $Y$ ignoring $X$.

This is an application of the Law of Total Probability.



## Marginal PMF (2)

With the $X$-by-$Y$ representation of joint PMF, the marginal PMF of $X$ is the row sums, marginal PMF of $Y$ is column sums (written in the *margins*): 

```{r}
#| echo: false

tib |> 
  mutate(rowsum = c("1/4", "1/2", "1/4")) |> 
  bind_rows(tibble(X = "", Y0 = "1/2", Y1 = "1/2", rowsum = ""))  -> expanded_tib

expanded_tib |>
  kableExtra::kbl(col.names = c("", "y = 0", "y = 1", " "), booktabs = T) |> 
  kableExtra::column_spec(c(1,3),border_right = T) |>
  kableExtra::column_spec(1,bold = T) |> 
  kableExtra::row_spec(1, extra_css = "border-top: 1px solid") |> 
  kableExtra::row_spec(4, extra_css = "border-top: 1px solid")
```

<!-- Plan: finish probability slides --> 
<!-- Think about problems for problem set -- starting writing it (must be done by tomorrow) -->
<!-- EOD: finish syllabus, publish canvas -->  


## Marginal PMF (3)

With the graphical representation, think about *sweeping the mass to the axis*:


```{r rglpmf, webgl = T}
#| output: false
```

## Conditional PMF

For RVs $X$ and $Y$, we can also talk about the **conditional PMF** of $Y$ at a value of $X$: 

$$f_{Y|X}(y|x) = \text{Pr}[Y = y \mid X = x] = \frac{\text{Pr}[X = x, Y = y]}{\text{Pr}[X = x]} = \frac{f(x, y)}{f_X(x)}$$
$\forall y \in \mathbb{R}$ and $\forall x \in \text{Supp}[X]$.

. . .

- Just like conditional probability for events
- Intuitively, take the joint probabilities where $X = x$ and scale them up by $\text{Pr}[X = x]$



## Conditional PMF (2)

With the $X$-by-$Y$ representation of joint PMF, you get the conditional PMF of $Y$ given $X = x$ by dividing each row by the row sum (i.e. the marginal probability of $X = x$): 

:::: {.columns}

::: {.column width="50%"}

$f(x, y)$

```{r plainjoint}
```

:::

::: {.column width="50%"}

[$f_{Y|X}(y |x )$]{style="text-align: center;"}

```{r}
#| echo: false
tibble(X = str_c("x = ", c(0, 1, 2)),
       Y0 = c("1/2", "1/2", "1/2"),
       Y1 = c("1/2", "1/2", "1/2")) |>
  kableExtra::kbl(col.names = c("", "y = 0", "y = 1"), booktabs = T) |> 
  kableExtra::column_spec(1,bold = T, border_right = T) |> 
  kableExtra::row_spec(1, extra_css = "border-top: 1px solid")
```


:::

::::


<!-- Plan: finish probability slides --> 
<!-- Think about problems for problem set -- starting writing it (must be done by tomorrow) -->
<!-- EOD: finish syllabus, publish canvas -->  


## Conditional PMF (3)

With the graphical representation, think about *taking a slice of the PMF and rescaling it*:


```{r rglpmf, webgl = T}
#| output: false
```

## Independence of random variables




